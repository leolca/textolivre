\documentclass[portuguese]{textolivre}

% metadata
\journalname{Texto Livre}
\thevolume{17}
%\thenumber{1} % old template
\theyear{2024}
\receiveddate{\DTMdisplaydate{2023}{12}{8}{-1}}
\accepteddate{\DTMdisplaydate{2024}{2}{23}{-1}}
\publisheddate{\DTMdisplaydate{2024}{4}{11}{-1}}
\corrauthor{Thiago Blanch Pires}
\articledoi{10.1590/1983-3652.2024.49118}
%\articleid{NNNN} % if the article ID is not the last 5 numbers of its DOI, provide it using \articleid{} commmand 
% list of available sesscions in the journal: articles, dossier, reports, essays, reviews, interviews, editorial
\articlesessionname{articles}
\runningauthor{Pimentel e Pires}
%\editorname{Leonardo Araújo} % old template
\sectioneditorname{Daniervelin Pereira}
\layouteditorname{João Mesquita}

\title{Treinamento e análise de um modelo de tradução automática baseado em \textit{Transformer}}
\othertitle{Training and analyzing a Transformer-based machine translation model}

\author[1]{Clóvis Henrique Martins Pimentel~\orcid{0009-0008-7049-7688}\thanks{Email: \href{mailto:clovismpimentel@gmail.com}{clovismpimentel@gmail.com}}}
\author[1]{Thiago Blanch Pires~\orcid{0000-0002-0060-6075}\thanks{Email: \href{mailto:thiagocomaga@gmail.com}{thiagocomaga@gmail.com}}}
\affil[1]{Universidade de Brasília, Brasília, DF, Brasil.}

%\usepackage{tabularx}
%\usepackage[english,french,brazilian]{babel}
% \usepackage{fourier} 
% \usepackage{array}
% \usepackage{makecell}
% \usepackage{csquotes}
% \renewcommand\theadfont{\normalsize}
\setotherlanguage{french}

\usepackage{listings}
\lstset{breaklines=true}
\lstdefinestyle{json}{
    basicstyle=\ttfamily,
    columns=fullflexible,
    showstringspaces=false,
    commentstyle=\color{gray}\upshape,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    frame=single,
}


\addbibresource{article.bib}

\begin{document}
\maketitle
\begin{polyabstract}
\begin{abstract}
O presente trabalho possui como objetivo a análise dos modelos de tradução automática baseados em \textit{Transformer}. Em específico, a pesquisa visa ao teste da viabilidade do uso de modelos treinados a partir de \textit{corpus} especializado. Para o treinamento do modelo, foi construído um \textit{corpus} paralelo inglês-francês a partir de sete textos da Convenção de 25 de outubro de 1980 sobre os Aspectos Civis do Rapto Internacional de Crianças. Os resultados de tradução obtidos pelo modelo treinado foram comparados com aqueles produzidos pelo Google Tradutor. Para a etapa de avaliação foram utilizados os métodos de avaliação automática sacreBLEU e avaliação humana. Os resultados da avaliação automática de frases produzidas pelo modelo treinado foram, em média, mais positivos que aqueles gerados pelo modelo não treinado. A avaliação humana das frases revelou que houve erros de adequação no uso da linguagem específica à matéria da Convenção da Haia de 1980 tanto em frases geradas pelo modelo treinado, quanto em frases geradas pelo modelo do Google Tradutor.

\keywords{Linguística Computacional \sep Tradutor automático \sep \textit{Transformer} \sep \textit{Corpus} paralelo \sep Avaliação de tradução automática}
\end{abstract}

\begin{english}
\begin{abstract}
The objective of this work is to analyze transformer-based machine translation models. It aims to test the feasibility of using trained models based on specialized \textit{corpus}. For the training of such model, a parallel English-French \textit{corpus} was built with seven texts related to the Convention of 25 October 1980 on the Civil Aspects of International Child Abduction. The translation results obtained by the trained model were compared with those produced by Google Translate. For the evaluation stage, sacreBLEU automatic evaluation and human evaluation methods were used. The outcome of the automatic evaluation of sentences produced by the trained model was, on average, higher than those generated by the non-trained model. The human evaluation of the sentences revealed that there were adequacy errors in the use of language specific to the subject matter of the 1980 Hague Convention both in sentences generated by the trained model and in sentences generated by the Google Translate model.

\keywords{Computational Linguistics \sep Machine translation \sep Transformer \sep Parallel \textit{corpus} \sep Machine translation evaluation}
\end{abstract}
\end{english}
\end{polyabstract}

\section{Introdução}
Em meados de 2017, com a publicação do artigo Attention is \textit{All You Need, pela equipe da Google} \cite{vaswani_attention_2017}, iniciou-se a mudança de paradigma das técnicas utilizadas no processamento de dados sequenciais, especialmente no campo do Processamento de Linguagem Natural (PLN). Uma nova arquitetura de rede neural denominada \textit{Transformer} possibilitou o aprimoramento significativo da qualidade de traduções automáticas multilíngues, antes amplamente baseada em modelos de processamento de redes neurais recorrentes (RNNs).

Até então, os tradutores automáticos baseados em RNNs eram dominantes, mas ainda apresentavam limitações na captura de dependências de longo prazo, resultando na perda de informações contextuais importantes \cite{iosifova_techniques_2020}. Com a chegada do \textit{Transformer}, a tradução automática passou a apresentar resultados significativamente melhores, possibilitando a criação de modelos com maior capacidade de lidar com contextos complexos e produzir traduções mais precisas (\textit{Ibid.}).

Atualmente, é possível encontrar de forma acessível tradutores automáticos que produzam traduções multilíngues de alta qualidade, ainda que com certas restrições em relação a sua confiabilidade \cite{pena_aguilar_challenging_2023} ou fluência \cite{banitz_machine_2020}. Por isso, áreas como os estudos de avaliação de tradução automática e da pós-edição de tradução automática são necessários para que se levante os principais problemas e possíveis soluções para o uso desses sistemas.

Embora seja notável a contribuição da pós-edição de tradução automática por meio de trabalhos como o de \textcite{koponen_is_2016} e de \textcite{ohagan_routledge_2019}, aplicada a áreas diversas como a tradução audiovisual para legendagem e a tradução literária \cite{costa_translation_2020,koglin_quality_2023}, os estudos de avaliação de tradução automática ainda são necessários como etapa de levantamento e análise de dados obtidos automática ou manualmente de resultados de sistema de tradução automática. São necessários, pois, além de revelarem um painel sobre a qualidade dos resultados, subsidiam reflexões críticas e profundas sobre determinados modelos e conjuntos de dados utilizados como treinamento de tais sistemas.

Ainda que os resultados de tradução automática de determinados sistemas, tais como Google Tradutor e DeepL sejam confiáveis para uma ampla gama de gêneros textuais e em vários pares linguísticos, eles podem vir a apresentar resultados menos satisfatórios quando comparados a modelos de tradução automática treinados para um propósito mais específico. O intuito deste trabalho é justamente verificar se há divergência de resultados entre esses dois modelos de tradução automática.

Diante dessa situação, surge a indagação sobre como seria possível desenvolver um tradutor multilíngue, baseado em uma arquitetura \textit{Transformer}, capaz de lidar com o léxico especializado. Assim, o presente trabalho busca preencher a lacuna de desenvolvimento de um modelo de tradução especializado no léxico utilizado em torno da Convenção de 25 de outubro de 1980 sobre os Aspectos Civis do Rapto Internacional de Crianças e Adolescentes \cite{hcch_convention_1980}.

Com efeito, busca-se testar a viabilidade do uso de um modelo de tradução automática baseado em \textit{Transformer}, levando em consideração a sua facilidade de implementação e a qualidade dos resultados gerados. Para a análise da qualidade, compara-se a precisão e adequação lexical dos resultados do modelo treinado em relação a resultados de traduções automáticas feitas por outras plataformas, como o Google Tradutor. Para tanto, a presente investigação compila um \textit{corpus} temático a partir do documento da Convenção de 25 de outubro de 1980 sobre os Aspectos Civis do Rapto Internacional de Crianças \cite{hcch_convention_1980}, que serve de base para o treinamento de um tradutor automático especializado.

Os textos que compõem o \textit{corpus} aqui desenvolvido têm caráter eminentemente jurídico, mais especificamente do Direito Internacional Público (DIP). Gravitam em torno dessa convenção, uma vez que é ela o texto jurídico base para a organização de um sistema internacional de cooperação entre Estados, medidas e ferramentas a serem utilizadas para a correta execução de ações coordenadas internacionalmente.

Em relação à metodologia (detalhada na seção \ref{sec-conduta} deste artigo), procura-se aplicar o modelo de tradução automática T5\footnote{T5 é um modelo codificador-decodificador pré-treinado em uma mistura de tarefas não supervisionadas e supervisionadas, em que cada tarefa é convertida para um formato de \textit{text-to-text}.}, introduzido pelo Google Research em 2019 \cite{raffel_exploring_2020} e disponibilizado gratuitamente pela plataforma \textit{HuggingFace}, especializada em processamento de linguagem natural (PLN) e aprendizado de máquina. As versões que compõem o \textit{corpus} constituído pela “Convenção sobre os Aspectos Civis do Sequestro Internacional de Crianças”, estão em inglês e francês – as duas línguas oficiais da Conferência da Haia de Direito Internacional Privado. Todo o conteúdo do \textit{corpus} foi retirado de textos e traduções oficiais disponibilizados no site dessa Conferência \cite{perez-vera_hcch_1980,hcch_convention_1980,hcch_guide_2003a,hcch_guide_2003b,hcch_guide_2005,hcch_guide_2010,hcch_guide_2012}.
 
Com o \textit{corpus} construído, aplicou-se o modelo de tradução automática preexistente, treinando-o para a obtenção de traduções que utilizassem de forma semanticamente mais precisa expressões com maior recorrência no âmbito do léxico da convenção internacional aqui mencionada (Mais detalhes na seção \ref{sec-conduta}). Ao final, as traduções obtidas foram analisadas e comparadas aos resultados gerados pelo Google Tradutor.

A seguir, a seção \ref{sec-normas} discute alguns conceitos técnicos e linguísticos mais relevantes e faz uma revisão de literatura, baseada em estudos que abarcam o tema aqui tratado. A seção \ref{sec-conduta} descreve com detalhes a metodologia utilizada para a produção dos resultados, que, por sua vez, são analisados na seção \ref{sec-modelo}. Por fim, as considerações finais explicitam a qualidade dos resultados obtidos e ressaltam sua relevância para a área.

\section{Revisão de literatura}\label{sec-normas}
Baseado no estado-da-arte da tecnologia de tradução automática, aqui ilustrado pelas pesquisas de \textcite{lakew_comparison_2018,banitz_machine_2020,tian_french--english_2022,iosifova_techniques_2020,kimera_building_2022}, opta-se pela utilização de um modelo pré-treinado baseado na arquitetura \textit{Transformer} para o processamento de seus dados. A escolha se justifica graças à comprovação da eficácia desse modelo em detrimento do uso de RNNs \cite{vaswani_attention_2017,wolf_huggingfaces_2020,raffel_exploring_2020}. Tal característica é confirmada por \textcite{lakew_comparison_2018}, que também atesta a superioridade que essa arquitetura possui quando desempenha tarefas de processamento de dados em modelos bilíngues, objeto aqui estudado.

Outro trabalho relevante é o modelo de tradução automática francês-inglês, baseado em uma arquitetura \textit{Transformer}, desenvolvido por \textcite{tian_french--english_2022}. O artigo \cite{tian_french--english_2022} trata da superioridade do modelo na performance de tarefas de tradução automática e afirma que modelos baseados em RNNs possuem dois grandes defeitos. O primeiro deles seria a sua limitação no processamento, restritos ao processamento individual de palavras e ocasionando um atraso em relação a sua velocidade de treinamento. O segundo ponto negativo mencionado em relação ao modelo que utiliza RNNs é a incapacidade dessa arquitetura de processar dados de forma precisa quando as frases se tornam demasiadamente longas.

Utilizando então um modelo baseado em \textit{Transformer}, \textcite{tian_french--english_2022} treinaram um modelo de tradução automática francês-inglês. Os resultados do treinamento puderam demonstrar uma acurácia de 80\% em suas traduções e informado como maior que aquele produzido por um modelo baseado em RNNs. O trabalho (\textit{Ibid.}) no entanto, se limita a explicitar somente os dados obtidos a partir do processo de treinamento do modelo (perda de validação e de treinamento). O artigo (\textit{Ibid.}) não demonstra os valores de acurácia do modelo RNNs e tampouco procede à avaliação automática ou humana de nenhuma tradução produzida pelo modelo treinado baseado em \textit{Transformer}.

O estudo de \textcite{iosifova_techniques_2020}, alinhado a \textcite{koponen_is_2016,costa_translation_2020}, parte do pressuposto de que um modelo de tradução automática produz traduções mais adequadas semanticamente quando submetido a um posterior processo de treinamento com dados que possibilitem a melhora do desempenho de uma tarefa específica. Diferentemente daquele \cite{iosifova_techniques_2020}, busca-se aqui a utilização do \textit{corpus} paralelo inglês-francês para a melhora na precisão do vocábulo utilizado em traduções sobre o tema da Convenção de Haia de 1980 \cite{hcch_convention_1980}.

Uma outra pesquisa que se aproxima mais do presente trabalho foi desenvolvida por \textcite{kimera_building_2022}. Em seu artigo, os autores (\textit{Ibid.}) descrevem a construção de um \textit{corpus} paralelo contendo 41.070 pares de frases em inglês e luganda, posteriormente usado para o treinamento de um modelo baseado em \textit{Transformer}. Após o treinamento de vários modelos testando hiperparâmetros diferentes, em um deles foi possível obter o score BLEU final de 17,47 para traduções de inglês para luganda.

Apesar de ainda contar com um número substancialmente maior de frases alinhadas e capacidade de processamento de dados igualmente superior, tal trabalho \cite{kimera_building_2022} lança luz sobre a realidade dos materiais necessários para se produzir um modelo de tradução automática minimamente capaz de melhorar um modelo pré-treinado não especializado. Esse trabalho (\textit{Ibid.}) auxilia na compreensão da relação entre o modo como o modelo é treinado e a aferição dos resultados produzidos pelo modelo. Durante o artigo são demonstrados alguns dados que comprovam a relação direta entre os hiperparâmetros escolhidos pelos autores (como o tamanho do \textit{batch}) e a influência que eles possuem na aferição do \textit{score} BLEU do modelo. Alterando alguns hiperparâmetros é possível obter \textit{scores} menores em relação ao \textit{score} final. O estudo (\textit{Ibid.}) demonstra, então, a definição de parâmetros não ótimos que leva ao \textit{score} BLEU 13,96.

Semelhante ao que se propõe o presente estudo, o artigo produzido por \textcite{kimera_building_2022}, enseja não somente apresentar o \textit{score} do modelo após seu treinamento, mas explora também o resultado de quatro frases traduzidas do inglês para luganda produzidas pelo seu modelo de tradução. Apesar disso, diferentemente do que se propõe neste artigo, o texto de \textcite{kimera_building_2022} não apresenta nenhuma avaliação humana consistente de suas traduções e nem pôde comparar, apesar de ter explicitado que esse era um dos objetivos futuros do trabalho, as traduções com resultados gerados pelo Google Tradutor. Tal fato se deve por ainda não existir, até a data da submissão deste artigo, a língua luganda disponível na plataforma \textit{online}.

Cabe, por fim, mencionar o trabalho de \textcite{banitz_machine_2020}, que realiza um estudo exímio na avaliação de traduções realizadas por dois modelos diferentes, um deles sendo o Google Tradutor e o outro o Systran. Em sua pesquisa, a autora utiliza tanto métodos de avaliação automática (\textit{Translation Error Rate}/TER \textit{score})\footnote{Apesar de justificar a escolha desse método de avaliação automática como um método mais intuitivo do “quão bom” é a tradução, a autora explicita as limitações impostas por essa métrica de avaliação como: (i) o método não reflete necessariamente a adequação da tradução gerada e (ii) a métrica depende diretamente da qualidade da tradução-referência, uma vez que qualquer desvio da tradução humana será penalizada \cite{banitz_machine_2020}.} como métodos de avaliação humana para avaliar as primeiras 24 frases traduzidas a partir de seu \textit{corpus}. Em relação à avaliação automática, a autora7esquematiza os \textit{scores} TER obtidos dos resultados de tradução dos dois mecanismos automáticos em uma tabela e os compara explicitando que as traduções produzidas pelo Google Tradutor requerem, ao final, menos pós-edição, uma vez que apresentam uma taxa de erro menor que o outro modelo avaliado. Já em relação à avaliação humana dos resultados, o trabalho de \textcite{banitz_machine_2020} utiliza métricas bem delineadas de avaliação de fluência e adequação das frases geradas (assunto explorado na subseção \ref{sec-formato}). Também em uma tabela comparativa dos resultados de tradução gerados pelos dois mecanismos de tradução automática, a autora compara os \textit{scores} atribuídos às frases e aponta os resultados do Google Tradutor como mais satisfatórios. Todavia, a mesma (\textit{Ibid.}) ainda explicita os desafios linguísticos que a tradução automática encontra em relação a erros semânticos, lexicais, sintáticos e morfológicos.
 
Apesar de não abordar o uso de modelo \textit{Transformer} especificamente treinados para a melhora do resultado de traduções, o trabalho de \textcite{banitz_machine_2020} abarca importantes discussões sobre os métodos de avaliação desenvolvidos e utilizados para a avaliação e comparação de traduções automaticamente geradas. É com base nestes aspectos que o trabalho da autora (\textit{Ibid.}) contribui para este artigo, fornecendo grande suporte científico para o método de comparação dos resultados aqui elaborado.

Para a melhor compressão dos parâmetros apresentados por \textcite{banitz_machine_2020}, é necessário entender que o erro de tradução, objeto de grande problematização dentro dos estudos de tradução, passa a ser utilizado no contexto da computação, com seu sentido matemático de “cômputo”, dificilmente sendo objeto de questionamentos \cite{pires_ampliando_2017}. Assim, os erros de tradução são aqui entendidos como “configurações de incompatibilidades linguísticas (lexical, semântica e sintática) entre o texto de entrada e o texto de saída gerado por uma tradução automática em um dado contexto de produção” (\textit{Ibid.}). \textcite[p. 242]{white_how_2003}, ao abordar o problema da falta de verdade absoluta na tradução, ressalta que o “método de comparação entre o resultado da tradução gerado automaticamente e traduções consideradas ‘corretas’, ainda que humano, é abstrato”.

Após a análise de alguns trabalhos relevantes na área, o presente estudo aborda na próxima seção a metodologia desenvolvida neste artigo para treinar um modelo de tradução automática baseado em \textit{Transformer}, levando em consideração o vocabulário específico da Convenção da Haia de 1980 \cite{hcch_convention_1980}. A próxima seção delineia técnicas específicas para o alcance de maior precisão, consistência e adaptação do léxico ao campo do conhecimento em questão.


\section{Metodologia}\label{sec-conduta}
Primeiramente, para a confecção do \textit{corpus} especializado no tema da Convenção sobre os Aspectos Civis do Rapto Internacional de Crianças \cite{hcch_convention_1980}, optou-se pela seleção de textos e traduções, obtidos online de forma gratuita, que fossem reconhecidos como oficiais pela própria Conferência da Haia de Direito Internacional Privado. A instituição é uma organização intergovernamental da área do Direito Internacional Privado (DIP) que administra diversas convenções internacionais, protocolos e instrumentos de \textit{soft law} (regras de valor normativo limitado e que não possuem caráter jurídico obrigatório), com o objetivo de unificar progressivamente as normas dessa área \cite{rodas_2007}.

Apesar de suas convenções internacionais não possuírem mandatoriamente o valor de lei para os países que assim não as convalidam, as Convenções e instrumentos fornecem clareza e direção em relações transfronteiriças com diversas matérias de DIP, dentre elas o Direito Internacional de Família e Proteção à Criança e Adolescente. É tarefa dessa Conferência uniformizar as normas aplicadas, zelar pelo seu adequado cumprimento e difundi-las entre os países. Com isso, a seleção de textos exclusivamente oficiais, produzidos pela própria Conferência da Haia, atribui ao \textit{corpus} uma maior segurança e uniformidade em seu léxico\footnote{Cf. \url{https://www.hcch.net/pt/about}. Acesso em: 17 jul. 2023.}.
 
Em relação à escolha das duas línguas selecionadas para integrar o \textit{corpus} paralelo, a opção pelas línguas inglês e francês se deu pela grande disponibilidade de material produzido nesses idiomas que, até o presente momento, são as duas únicas línguas oficiais dessa organização intergovernamental\footnote{Cabe destacar que apesar de o site já possuir versões completas em outras línguas como o alemão, português e espanhol, somente essa última será introduzida em 1 de julho de 2024, ao rol de línguas oficiais da Conferência. \url{https://www.hcch.net/pt/news-archive/details/?varevent=907}. Acesso em: 13 jul. 2023.}. Assim, a maioria dos textos possui sua primeira redação em francês, com sua subsequente tradução oficial para o inglês e posterior disponibilização em outros idiomas\footnote{Para a construção do \textit{corpus} foram utilizados, ao todo, 7 pares de textos, cada um com sua versão original em francês e versão em inglês: (i) texto oficial da Convenção sobre os Aspectos Civis do Rapto Internacional de Crianças \cite{hcch_convention_1980}; (ii) Relatório Explicativo de Eliza Pérez-Vera (Pérez-Vera, 1980); e (iii) Guias de Boas Práticas sobre a Convenção da Haia de Subtração Internacional de Crianças e Adolescentes (Partes I a V) \cite{hcch_guide_2003a,hcch_guide_2003b,hcch_guide_2005,hcch_guide_2010,hcch_guide_2012}. Textos disponíveis em \url{https://www.hcch.net/pt/publications-and-studies/publications2}. Acesso em: 12 jul. 2023.}. Após a seleção dos textos, foram necessárias a limpeza e formatação dos textos e alinhamento semântico dos pares de frases, feitas de forma manual utilizando planilha do Microsoft Excel, conforme \Cref{tab-01}.


\begin{table}[!htpb]
\centering
\begin{threeparttable}
\caption{Frases em inglês e francês alinhadas de acordo com sua correspondência semântica.}
\label{tab-01}
\begin{tabular}{p{6cm}p{6cm}}
\toprule
Texto em Inglês & Texto em Francês\\
\midrule
\foreignlanguage{english}{Convention on the Civil Aspects of International Child Abduction} &
Convention Sur Les Aspects Civils De L'enlèvement International D'enfants\\
Concluded 25 October 1980 &
Conclue le 25 octobre 1980\\
The States signatory to the present Convention, &
Les Etats signataires de la présente Convention,\\
Firmly convinced that the interests of children are of paramount importance in matters relating to their custody, &
Profondément convaincus que l'intérêt de l'enfant est d'une importance primordiale pour toute question relative à sa garde,\\
Desiring to protect children internationally from the harmful effects of their wrongful removal or retention and to establish procedures to ensure their prompt return to the State of their habitual residence, as well as to secure protection for rights of access, &
Désirant protéger l'enfant, sur le plan international, contre les effets nuisibles d'un déplacement ou d'un non-retour illicites et établir des procédures en vue de garantir le retour immédiat de l'enfant dans l'Etat de sa résidence habituelle, ainsi que d'assurer la protection du droit de visite,\\
Have resolved to conclude a Convention to this effect and have agreed upon the following provisions. &
Ont résolu de conclure une Convention à cet effet, et sont convenus des dispositions suivantes.\\
\bottomrule
\end{tabular}
\source{Elaboração própria.}
\end{threeparttable}
\end{table}	


Foi necessário realizar a limpeza e o alinhamento de quase 5.500 frases pareadas em inglês e francês. Levando em conta todos os textos utilizados, é possível ainda documentar, aproximadamente, 278.000 \textit{tokens} (total de ocorrências) e 15.500 \textit{types} (vocábulos distintos) presentes entre os textos em inglês e francês.

O próximo passo foi decidir a plataforma que seria utilizada para o processamento desses dados e notou-se que a \textit{HuggingFace} seria a melhor opção para isso. Como justificativa, sublinhamos se tratar de uma ferramenta oferecida de forma gratuita e que disponibiliza uma ampla gama de modelos pré-treinados e bibliotecas (inclusive a que disponibiliza o modelo \textit{Transformer}) que facilitam o desenvolvimento, treinamento e implantação de modelos de tradução automática. Com todos esses recursos disponíveis, a própria plataforma ensina, por meio de um tutorial utilizando a linguagem de programação \textit{Python}, a implementar a sua biblioteca \textit{Transformer}.

Para que fosse possível utilizar o \textit{corpus} paralelo EN-FR, foi criado um dicionário em arquivo JSON com os pares de frases indexados, conforme exemplo a seguir:

\begin{lstlisting}[style=json, caption={Amostra de dicionário em formato JSON relativo a \textit{corpus} paralelo EN-FR}, label=lst01]
[{"id": "0", "translation": {"en": "Convention on the Civil Aspects of International Child Abduction", "fr": "Convention Sur Les Aspects Civils De L'enlèvement International D'enfants"}}]
\end{lstlisting} %stopzone

% \begin{quote}
%     \\
%     \lstinline[basicstyle=\ttfamily]@[{"id": "0", "translation": {"en": "Convention on the Civil Aspects of International Child Abduction", "fr": "Convention Sur Les Aspects Civils De L'enlèvement International D'enfants"}}]@
%     %\verb|[{"id": "0", "translation": {"en": "Convention on the Civil| \verb|Aspects of International Child Abduction", "fr": "Convention| \verb|Sur Les Aspects Civils De L'enlèvement International D'enfants"}}]| 
%     \\
%     Fonte: Autores
% \end{quote}




Em relação à escolha do modelo de \textit{Transformer} utilizado a partir da biblioteca gratuita disponibilizada pela \textit{HuggingFace}, optou-se por um modelo pré-treinado e que não demandasse uma grande capacidade de processamento computacional para o seu treinamento a partir do \textit{corpus} especializado. A escolha teve como base o modelo “SEBIS/legal\_t5\_small\_multitask\_en\_fr" de tradução inglês-francês para textos de conteúdo jurídico, disponível gratuitamente na plataforma \textit{HuggingFace}. Assim, foi selecionado o T5-\textit{small}, um modelo de rede neural pré-treinado para tarefas de \textit{text-to-text}, adequado para tarefas de tradução automática aplicadas a máquinas com menor capacidade de processamento \cite{raffel_exploring_2020}\footnote{Esse modelo foi escolhido por sua capacidade de lidar com uma ampla gama de tarefas de PLN, inclusive traduções multilíngues, e, por já se encontrar em uma fase de pré-treinamento, ser necessário somente o treinamento desse modelo em relação ao léxico especializado da Convenção da Haia de 1980 \cite{raffel_exploring_2020}.}.

No entanto, alguns ajustes foram feitos, já que o modelo SEBIS foi treinado a partir de um \textit{corpus} paralelo de 9 milhões de pares de frases, 220 milhões de parâmetros, \textit{batch size} de tamanho 4096, \textit{sequence length} de 512, e \textit{corpus} pré-processado a partir de 88 milhões de frases, com \textit{score} sacreBLEU de 38,063. Os números indicam a necessidade de uma máquina com grande capacidade de processamento, superior ao que se é possível realizar em um computador de uso pessoal. Em decorrência disso, para o presente trabalho, foram redefinidos os valores do \textit{epoch} (número de vezes que todo o conjunto de treinamento é percorrido durante o treinamento) para cinco e dos \textit{batches} (quantidade de blocos de pares de frases analisados pelo modelo a cada iteração) para 16, sendo possível obter, após 3 horas 46 minutos e 36 segundos, um modelo de tradução treinando especializado no vocabulário referente à Convenção sobre os Aspectos Civis do Rapto Internacional de Crianças \cite{hcch_convention_1980}.

Partindo-se então do código-exemplo\footnote{Cf. \url{https://HuggingFace.co/docs/transformers/tasks/translation}. Acesso em: 12 jul. 2023.}, que produz seus resultados a partir de uma arquitetura híbrida de RNNs e \textit{Transformer}, o \textit{Google Neural Machine Translation} (GNMT) \cite{wu_googles_2016} de implementação da biblioteca \textit{Transformer} presente na plataforma \textit{HuggingFace} e utilizando o arquivo JSON previamente mencionado, procedeu-se às adaptações do código para o processamento local do \textit{corpus} paralelo utilizado para o treinamento de um novo modelo especializado de tradução. Aqui, 20\% do \textit{corpus} (1.099 frases) foi utilizado para teste e o restante (4.395 frases) para o treinamento do modelo. Após o treinamento do modelo, os resultados obtidos foram comparados com traduções realizadas pelo Google Tradutor\footnote{Testes realizados em 29 de junho de 2023, às 17h22min, horário local de Brasília, Brasil.}.

Buscou-se, assim, avaliar a eficácia do modelo de tradução treinado a partir de um \textit{corpus} especializado em relação aos resultados produzidos pelo tradutor online. Com essa metodologia, foi possível aferir a capacidade do modelo criado a partir de um \textit{corpus} especializado em produzir traduções linguisticamente precisas e contrastar seus resultados com aqueles produzidos pelo GNMT.

\subsection{Avaliação Automática}\label{sec-fmt-manuscrito}
Para a aferição dos resultados optou-se pela utilização de dois métodos: (i) métrica de comparação automática e (ii) avaliação humana, esta última abordada na próxima subseção deste trabalho. Em relação à métrica de comparação automática, utilizou-se a ferramenta de código aberto sacreBleu, que desempenha o cálculo do \textit{score} BLEU para avaliar a qualidade das traduções automáticas em comparação com as referências humanas \cite{post_call_2018,papineni_bleu:_2002}.

A metodologia adotada permite uma avaliação objetiva e sistemática da qualidade das traduções automáticas, comparando-as com referências humanas e levando em consideração a precisão dos \textit{n-gramas} (sequência de n itens – por exemplo, palavras – de uma amostra de texto). Contudo, assim como o \textit{score} BLEU, o sacreBLEU também possui limitações que devem ser consideradas ao interpretar os resultados aqui obtidos, como a sua falta de compreensão semântica e as limitações impostas pelas referências que ele utiliza para avaliar a tradução\footnote{O \textit{score} também apresenta outras limitações (insensibilidade à ordem das palavras e sintaxe, ênfase excessiva na correspondência do número de n-gramas, favorecimento de traduções mais curtas insensibilidade a sinônimos e paráfrases), essas não tão relevantes para a análise dos resultados desse trabalho \cite{post_call_2018}.}.

Em relação à falta de compreensão semântica, o BLEU não avalia a precisão das traduções, não sendo capaz capturar as nuances semânticas cruciais para traduções que requerem o uso de termos especializados. É possível que uma tradução tenha o mesmo significado que uma referência utilizada pela própria métrica, mas receba um \textit{score} baixo. Da mesma forma, é possível que uma tradução tenha altas pontuações de BLEU, mas transmita um significado diferente ou incorreto em comparação com as referências (caso ilustrado pela \Cref{tab-01}, na seção \ref{sec-modelo}).

No caso das limitações impostas pelas referências que a métrica BLEU utiliza para avaliar a tradução, ela depende de frases-referência definidas por humanos para a avaliação. A escolha das referências pode ser subjetiva e não capturar toda a variedade de traduções aceitáveis. Além disso, os resultados gerados nem sempre melhoram com o aumento do número de frases-referência, e estudos recentes comprovam melhor desempenho do \textit{score} quando somente uma frase-referência foi utilizada \cite{freitag_bleu_2020}. 

Ao utilizar a avaliação automática também para avaliar não só o modelo, mas frases produzidas por ele, foram realizadas avaliações de frases individuais produzidas tanto pelo Google Tradutor quanto pelo modelo treinado. Seguindo a técnica utilizada por \textcite{freitag_bleu_2020}, para cada tradução comparada, selecionou-se a frase correspondente do texto oficial em francês como parâmetro de referência utilizado pelo sacreBLEU para a avaliação automática e geração de \textit{scores} individuais. Os resultados foram posteriormente comparados, seguindo modelo de formatação utilizado por \textcite{banitz_machine_2020}, oportunamente apresentado na seção dedicada à discussão dos resultados.

Por optar, então, pela utilização de uma métrica de avaliação automática de fácil implementação e amplamente difundida, selecionou-se o sacreBLEU como parâmetro de avaliação automática. Necessário, entretanto, lembrar que cada métrica de avaliação encontra algum tipo de limitação que deve ser considerada quando utilizada na avaliação automática de modelos de tradução de mesma natureza. Em decorrência disso, o presente trabalho prezou também pelo desenvolvimento da avaliação humana, que se torna imprescindível para a obtenção de uma compreensão mais abrangente da qualidade da tradução gerada pelo modelo treinado, assunto tratado a seguir.

\subsection{Avaliação Humana}\label{sec-formato}
A base da avaliação humana adotada é constituída especialmente pelos trabalhos de \textcite{banitz_machine_2020,vilar_error_2006}. O primeiro utiliza os parâmetros de fluência e adequabilidade para aferir a qualidade das traduções, enquanto o segundo apresenta a classificação (com respectivas subclasses) de uma série de erros cometidos pela máquina ao realizar uma tradução.

Primeiramente, em relação aos termos utilizados pela autora \textcite{banitz_machine_2020}, ela explica que a fluência de uma tradução gerada também pode ser entendida como o nível de sua inteligibilidade, compreendendo tanto a correção gramatical quando a escolha de palavras utilizadas na tradução \cite{douglas_machine_1994,kalyani_evaluation_2014}. Por sua vez, a adequação pode ser também entendida como a acurácia ou fidelidade da tradução produzida, e se relaciona com o grau no qual a tradução consegue representar o significado original da frase traduzida \cite{douglas_machine_1994,kalyani_evaluation_2014}.

Para a metrificação de cada um dos dois parâmetros, \textcite[p. 63]{banitz_machine_2020} descreve a atribuição de um valor que varia de 1 a 5, cabendo a um humano avaliar a frase traduzida com base nesses valores. Para a fluência, a atribuição do valor 1 corresponde a incompreensibilidade da frase e o valor 5 a uma frase perfeitamente inteligível em determinada língua. Em relação à adequação, atribuir o valor 1 a uma frase implica em dizer que o significado expressado nela não se aproxima de forma alguma do significado expressado na frase que se pretendia traduzir. Em contrapartida, o valor 5 expressa que não houve perda de nenhum significado na frase traduzida.

Em relação ao trabalho produzido por \textcite{vilar_error_2006}, esse apresenta uma série de categorias e subcategorias de erros passíveis de identificação quando da análise de uma tradução gerada automaticamente. Este trabalho, no entanto, opta por focar em uma subcategoria específica apresentada pelos autores: o erro aferido a partir da utilização errônea de palavras na tradução, especialmente quando levado em consideração o sentido da palavra presente o texto gerado. Isso pode ocorrer, de acordo com os autores, por uma escolha lexical errada ou por uma desambiguação incorreta. O presente estudo se atém ao fator relacionado a uma escolha lexical errada para comparar as traduções geradas pelo modelo treinado e aquelas geradas pelo Google Tradutor.
 
Para a execução da investigação proposta, o primeiro autor deste trabalho, bacharel em Direito e em Línguas Estrangeiras Aplicadas ao Multilinguismo e à Sociedade da Informação (LEA-MSI), com experiência profissional em DIP e atuação perante a Autoridade Central Administrativa Federal (ACAF)\footnote{Órgão, no Brasil, incumbido da adoção de providências para o adequado cumprimento das obrigações impostas pela Convenção de Haia de 1980 sobre os Aspectos Civis da Subtração Internacional de Crianças. (cf. Decreto nº 11.348, de 1º de janeiro de 2023; Decreto no 3.413, de 14 de abril de 2000.)}, utilizou os parâmetros anteriormente mencionados para proceder à avaliação humana da matéria aqui explorada. Partindo dessa experiência, é possível constatar que os fenômenos linguísticos gerados por sistemas de tradução automática têm relação direta na atuação da interface entre bacharel em LEA-MSI, já que se informa de fundamentos linguísticos, relações multilíngues e computacionais, desenvolvendo sua formação em tarefa especializada e combinada com o léxico especializado em Direito. Assim, conforme \textcite[p. 73]{pires_avaliacao_2020}, essa formação acadêmica pode proporcionar que tais profissionais atuem “acadêmica e profissionalmente de forma a olhar e desempenhar, com viés crítico e prático, o fenômeno linguístico que se coloca na relação complexa entre o humano e a máquina”.

Desta feita, utilizando os métodos de avaliação humana aqui expostos e as métricas de avaliação automática anteriormente discutidos, passa-se a apresentar e discutir, na próxima seção, os resultados obtidos por meio deste estudo.


\section{Resultados}\label{sec-modelo}
Para o treinamento do modelo de tradução automática, o \textit{corpus} paralelo construído foi composto por quatorze textos, sendo sete pares de textos em inglês-francês. Os textos selecionados foram todos retirados do site oficial da Conferência da Haia sobre Direito Internacional Privado. Um desses pares de textos representa o texto da própria Convenção de 25 de outubro de 1980 sobre os Aspectos Civis do Rapto Internacional de Crianças e Adolescentes \cite{hcch_convention_1980}. Os outros pares de textos selecionados possuem relação direta com essa Convenção, sendo um relatório explicativo sobre a própria convenção e outros cinco guias de boas práticas também relacionados à implementação dessa convenção internacional \cite{perez-vera_hcch_1980,hcch_convention_1980,hcch_guide_2003a,hcch_guide_2003b,hcch_guide_2005,hcch_guide_2010,hcch_guide_2012}.

Após a coleta dos textos, foram computados 134.943 \textit{tokens} e 6.453 \textit{types} presentes nos textos em inglês e 143.285 \textit{tokens} e 8.932 \textit{types} em francês. Ao final foram alinhadas 5.494 frases em inglês-francês. A partir disso, o modelo treinado recebeu o \textit{score} sacreBLEU de 7,6467.

Levando em consideração o tamanho do \textit{corpus} usado para treino neste modelo (5.494 pares de frases), e os hiperparâmetros ajustados de forma a ser possível o processamento em uma máquina com configurações feitas primariamente para o uso doméstico, vemos que o valor do resultado da avaliação automática encontra-se alinhado com os estudos previamente mencionados aqui \cite{lakew_comparison_2018,banitz_machine_2020,tian_french--english_2022,kimera_building_2022}.

Apesar de apresentar um \textit{score} de aproximadamente 7,65, o modelo tende a render melhor desempenho em sua tarefa de acordo com aquilo para o qual foi treinado. Assim, deve-se também avaliar individualmente a pontuação de alguns de seus resultados de frases traduzidas, para que seja possível compreender o que esse valor atribuído ao modelo, apesar de baixo, representa quando da realização de tarefas de tradução automática sobre o tema aqui exposto.
 
Para a análise dos resultados da avaliação automática, foram identificadas 20 frases em inglês (retiradas do par inglês-francês presente no \textit{corpus}) sustentadas pelos dois modelos de tradução por meio do número de sua chave correspondente no \textit{corpus}. Os \textit{scores} sacreBLEU gerados para cada tradução em francês proveniente de cada modelo seguem discriminados em colunas paralelas às chaves de identificação das frases. Para a obtenção dos \textit{scores}, cada tradução gerada, tanto pelo modelo treinado quanto pelo Google Tradutor, foi comparada com a frase-referência em francês presente no \textit{corpus}. A investigação, então, passou a analisar os scores individuais dessas frases traduzidas a partir do \textit{corpus} construído, conforme \Cref{tab-02}.

\begin{table}[!htpb]
\centering
\begin{threeparttable}
\caption{Comparação dos scores sacreBLEU atribuídos às frases geradas pelo modelo treinado e pelo Google Tradutor.}
\label{tab-02}
\begin{tabular}{lll}
\toprule
Chave & \multicolumn{1}{p{3cm}}{Score sacreBLEU Modelo Treinado} & \multicolumn{1}{p{3cm}}{Score sacreBLEU Google Tradutor} \\
\midrule
316 & 32,5 & 32,6 \\
384 & 29,5 & 58,1 \\
635 & 71,9 & 50,7 \\
796 & 22,8 & 23,2 \\
852 & 8,3 & 6,3 \\
950 & 20,6 & 12,6 \\
965 & 21 & 63,9 \\
1013 & 82,4 & 91,2 \\
1166 & 7,3 & 7,1 \\
1377 & 22 & 26,3 \\
1390 & 33 & 7,8 \\
1399 & 49,2 & 11,4 \\
1411 & 49,8 & 36,1 \\
1418 & 23,4 & 10 \\
1437 & 5,7 & 14,1 \\
1451 & 14,4 & 9,4 \\
1455 & 29 & 24,6 \\
1471 & 37,5 & 33,5 \\
1486 & 41,4 & 34,8 \\
1520 & 31.9 & 27,2 \\
Média & 31,7 & 29,0 \\
\bottomrule
\end{tabular}
\source{Elaboração própria.}
\end{threeparttable}
\end{table}


A partir dos resultados obtidos pela avaliação automática, pode-se perceber que o modelo treinado ainda é capaz de obter \textit{scores} maiores em relação ao modelo de tradução disponibilizado pela Google. O resultado da média dos \textit{scores} obtidos dentre as traduções geradas pelo modelo treinado foi de aproximadamente 31,66. Em contrapartida, o resultado da média dos \textit{scores} obtidos dentre as traduções geradas pelo Google Tradutor foi de 29,045. A diferença não é grande, mas dificilmente o seria, caso levássemos em consideração o tamanho do \textit{corpus} paralelo utilizado para o treinamento do modelo de tradução automática.

Apesar de o foco desta pesquisa ser o treinamento de um modelo de tradutor automático baseado em \textit{Transformer}, uma breve análise das traduções geradas demonstra-se relevante para a compreensão geral do \textit{score} sacreBLEU do modelo treinado. Para a avaliação humana, foi imperativo que essa tenha sido realizada por um profissional que atua na área de tradução e com experiência em matéria de DIP, sendo capaz de transitar entre os dois campos do conhecimento, aplicando teoria e prática tradutória ao léxico especializado que requer a matéria de subtração internacional de crianças e adolescentes dentro do DIP.

Após a análise humana das traduções geradas baseada nos parâmetros já descritos na subseção \ref{sec-formato}, pode-se afirmar que, apenas duas traduções geradas apresentaram erros, sendo uma delas gerada pelo modelo treinado e a outra pelo Google Tradutor. Todas as outras não apresentam erros relevantes que influenciassem na adequação e fluência da frase, podendo ser consideradas boas traduções. Em outras palavras, textos que aplicam de forma acertada termos jurídicos e jargões próprios da área relativa à matéria de subtração internacional de crianças e adolescentes.

Observando a \Cref{tab-03}, pode-se perceber que, a partir da frase em inglês, a tradução gerada pelo modelo Transformer treinado foi adequada e fluente. Houve, de fato, um erro \cite{vilar_error_2006} correspondente à ausência do artigo indefinido une, porém que não afeta a fluência e adequação do restante da frase \cite{banitz_machine_2020}. Essa foi a única diferença que impediu que a frase traduzida ficasse idêntica à frase francesa oficial usada como referência (presente no \textit{corpus} paralelo). A frase recebeu a pontuação de 32,5.

\begin{table}[!htpb]
\centering
\begin{threeparttable}
\caption{Resultado das traduções do modelo treinado e do Google Tradutor.}
\label{tab-03}
\begin{tabular}{*{4}{p{3.5cm}}}
\toprule
Frase EN para tradução & FR Referência & Resultado Tradução Modelo & Resultado Tradução Google\\
\midrule
\foreignlanguage{english}{Convention of cooperation among authorities}
[\textit{Convenção de cooperação entre autoridades}] &
\foreignlanguage{french}{Une convention de coopération entre autorités} 
[\textit{Uma convenção de cooperação entre autoridade}]&
\foreignlanguage{french}{Convention de coopération entre les autorités}
[\textit{Uma convenção de cooperação entre as autoridades}] &
\foreignlanguage{french}{Convention de coopération entre collectivités}
[\textit{Uma convenção de cooperação entre as autoridades territoriais}]\\
\bottomrule
\end{tabular}
\source{Elaboração própria.}
\end{threeparttable}
\end{table}

Por outro lado, a tradução do Google Tradutor gerou um resultado considerado fluente, porém inadequado para o contexto, uma vez que utiliza o termo “collectivités” para se referir aos órgãos federais de cooperação internacional relativa à Convenção Internacional de Haia de 1980 \cite{hcch_convention_1980} – as Autoridades Centrais. No entanto, o termo “collectivité” faz menção a departamentos administrativas reconhecidas na França, consideradas entidades territoriais coletivas, com poder de governo estabelecido. A terminologia engloba qualquer área que possua uma forma de governo local eleito e autoridade reguladora local e não faz referência à Autoridade Central, órgão incumbido da adoção de medidas para o cumprimento das obrigações impostas pela Convenção de Haia de 1980 \cite{hcch_convention_1980}.

 Ao utilizar a classificação de \textcite{vilar_error_2006}, este erro cometido pelo modelo de tradução do Google é claramente uma escolha lexical errada, afetando toda a adequação da tradução \cite{banitz_machine_2020}. Ainda assim, essa tradução gerada pelo Google Tradutor recebeu uma pontuação de 32,6, infimamente maior que a tradução mais correta gerada pelo modelo treinado.
 
Em relação à segunda frase que apresentou um erro, ela foi gerada pelo modelo treinado com base em \textit{corpus} especializado. Como demonstrado na \Cref{tab-04}, o modelo traduziu erroneamente a palavra “\textit{Requesting}” para “\textit{requises}”, trocando totalmente o sentido da frase em inglês\footnote{Importante aqui ressaltar que a Autoridade Central pode figurar tanto como Requerente (\textit{Requérant/Requesting}) como Requerida (\textit{Requise/Requested}). Esse título é importante para identificar onde o processo se inicia e qual o papel dessas Autoridades para a recuperação da criança ou adolescente.}. Assim, apesar de mantida a fluência da frase, não há adequação do termo utilizado \cite{banitz_machine_2020} e a escolha por esse léxico distorce o sentido da frase no idioma de entrada, podendo ser caracterizado como um resultado proveniente de uma escolha equivocada de léxico \cite{vilar_error_2006}. O \textit{score} sacreBLEU obtido para essa tradução foi de 5,7.

\begin{table}[!htpb]
\centering
\begin{threeparttable}
\caption{Resultado das traduções do modelo treinado e do Google Tradutor.}
\label{tab-04}
\begin{tabular}{*{4}{p{3.5cm}}}
\toprule
Frase EN para tradução &
Frase FR Referência &
Resultado Tradução Modelo &
Resultado Tradução Google\\
\midrule
\foreignlanguage{english}{Requesting Central Authorities are often under pressure from applicants (usually left-behind parents) to provide daily reports of progress}
[\textit{Autoridades Centrais requerentes geralmente estão sob pressão dos demandantes (geralmente os genitores que foram deixados para trás) para fornecer relatórios diários de progresso}] &
\foreignlanguage{french}{Les demandeurs (généralement les parents privés de leur enfant) exercent souvent des pressions sur les Autorités centrales pour qu’elles leur fournissent des rapports de suivi journaliers}
[\textit{Os demandantes (geralmente os pais privados de seus filhos) geralmente fazem pressão nas Autoridades Centrais para que elas lhes forneça relatórios diários de monitoramento}] &
\foreignlanguage{french}{Les Autorités centrales requises sont souvent soumises à des pressions de la part des demandeurs (habituellement des parents laissés derrière eux) pour fournir des rapports quotidiens sur les progrès accomplis}
[\textit{As autoridades centrais requeridas estão frequentemente sob pressão dos demandantes (geralmente os pais deixados para trás) para fornecer relatórios diários de progresso}] &
\foreignlanguage{french}{Les Autorités centrales requérantes subissent souvent des pressions de la part des demandeurs (généralement des parents délaissés) pour qu'elles fournissent des rapports quotidiens sur les progrès}
[\textit{As Autoridades Centrais Requerentes são frequentemente pressionadas pelos demandantes (geralmente pais deixados para trás) a fornecer relatórios diários de progresso}]  \\
\bottomrule
\end{tabular}
\source{Elaboração própria.}
\end{threeparttable}
\end{table}

Na análise da \Cref{tab-04}, em relação a tradução do Google dessa mesma frase, constata-se que ela foi igualmente fluente. Contudo, a sentença pode ser considerada mais adequada semanticamente ao contexto por ter utilizado corretamente o termo “\textit{requérantes}” para se referir às Autoridades Centrais Requerentes (\textit{Requesting}); termo também utilizado na frase-referência oficial em francês. O \textit{score} acompanhou essa lógica correta e atribuiu, à tradução automática do Google, 14,1 pontos.

A partir do exame de todos esses resultados gerados, é possível perceber que um modelo treinado com apenas 5.494 pares de frases consegue desempenhar tarefas de tradução automática baseada em vocábulo especializado com uma qualidade um ligeiramente melhor que os outros modelos. 
 
No que tange às avaliações automática e manual, é possível depreender dos resultados que a disparidade entre os resultados obtidos pelo modelo treinado e pela ferramenta de tradução do Google não diferem muito entre si. A avaliação automática ressalta a proximidade de termos e estruturas utilizadas por ambas as traduções, ao passo que a avaliação manual ratifica a qualidade das mesmas ressaltando apenas um equívoco no uso de léxico especializado gerado por cada ferramenta de tradução automática.

\section{Considerações finais}\label{sec-organizacao}
Este artigo teve como objetivo geral o treinamento e análise de modelos de tradução automática baseados em \textit{Transformer}, cujos resultados demonstram que a análise foi realizada a contento. Em um de seus objetivos específicos, o trabalho buscou o teste da viabilidade do uso de modelos de tradução automática baseados em \textit{Transformer}. A partir disso, foi possível perceber que a implementação do modelo foi facilitada pelo uso de código aberto e de bibliotecas disponibilizadas de forma gratuita pela plataforma \textit{HuggingFace}, fatores que contribuíram significativamente para a etapa inicial de escrita do código utilizado.

Em etapas mais avançadas da implementação do modelo, encontrou-se dificuldades de treinamento do modelo devido aos limites computacionais de capacidade de processamento de dados pela máquina utilizada. Como ilustrado com o modelo SEBIS, modelos de tradução automática baseados em \textit{Transformer} que contam com bons \textit{scores}, apesar de melhor aproveitarem a capacidade de processamento da máquina, ainda necessitam de uma quantidade significativa de dados para que possam ser treinados e, consequentemente, de máquinas que possam lidar de forma eficiente com esses dados. Com menor capacidade de processamento, o modelo pré-treinado T5-\textit{small}, os hiperparâmetros definidos e o tempo necessário para o treinamento se tornaram grandes limitadores para o desenvolvimento de um modelo de tradução automática que utiliza um léxico mais preciso e adequado em relação ao termos utilizados por essa área do DIP.

Em relação aos erros apontados na fase de avaliação de modelo, em específico na avaliação humana, constatou-se que ambos os modelos de tradução, seja ele treinado ou não, estão suscetíveis a falhas. Dessa forma, cabe melhorar o modelo \textit{Transformer} treinado para que seus resultados sejam ainda melhores. A revisão do \textit{corpus}, o aumento no número de pares de frases alinhadas, o investimento em máquinas e técnicas de processamento de dados mais avançadas, a dedicação de maior tempo para o treinamento do modelo e o teste de mais alguns hiperparâmetros ajustáveis no momento de treinar o modelo são só alguns dos fatores concretos apontados por esse trabalho que garantem uma melhora no modelo e, consequentemente, nas traduções geradas.

Não obstante, este trabalho iniciou o desenvolvimento de um \textit{corpus} paralelo inglês-francês específico relativo à Convenção da Haia de 1980 \cite{hcch_convention_1980}, sendo desenvolvido um conjunto de dados eminentemente promissor que serve de referência para pesquisas futuras e aplicações em trabalhos que busquem desenvolver técnicas semelhantes. O trabalho de organização, limpeza e anotação dos textos selecionados aqui realizado e documentado demonstram o rigor metodológico empreendido que, por sua vez, fornece os alicerces necessários para a revisão e expansão do \textit{corpus} relativo à convenção \cite{hcch_convention_1980}.

O trabalho ainda serviu como forma de avaliação da medida na qual a construção de um \textit{corpus} específico e a sua implementação em um modelo de tradução se torna mais satisfatória do que uma tradução feita em um modelo com léxico não especializado. É importante ressaltar o fator das dimensões envolvidas no trabalho, uma vez que quanto maior a quantidade de texto disponível para alimentar o treinamento modelo, maior a qualidade dos resultados esperada. Dessa forma, apesar de ter seu treinamento baseado em um \textit{corpus} que conta com somente 5.494 pares de palavras, o modelo de tradução desenvolvido se demonstra extremamente eficiente e desempenha, na maioria das vezes, melhor do que um modelo de tradução não treinado.

Em diversas etapas, que partem desde a concepção e elaboração de um \textit{corpus} paralelo em meio digital, e na anotação e programação de dados não-estruturados até a análise humana da tradução automática realiza, foi fundamental a formação prévia do primeiro autor em Línguas Estrangeiras Aplicadas ao Multilinguismo e à Sociedade da Informação, uma área do saber que tornou possível o desenvolvimento do trabalho e da análise dos resultados obtidos \cite{pires_avaliacao_2020}.

Para que a pesquisa aqui desenvolvida continue sendo relevante e sirva de base para acadêmicos não só ligados ao bacharelado em LEA-MSI, como também profissionais e pesquisadores ligados a outras áreas, como linguística, estudos da tradução, e ciência da computação, por exemplo, propõe-se a ampliação deste estudo tanto em tarefas de pós-edição, quanto em avaliações conjuntas (\textit{shared tasks}) \cite{freitas_linguistica_2022}. Por meio de pesquisas encabeçadas por especialistas em linguística e estudos da tradução, temos como objetivo a constante melhora do modelo aqui desenvolvido \cite{freitas_linguistica_2022}.

Por fim, considera-se que os resultados aqui relatados estejam alinhados com aquele produzidos por pesquisas semelhantes, com real possibilidade de melhora a partir do desdobramento, ampliação e aprofundamento de investigação de fatores especificamente apontados ao longo do trabalho.

\printbibliography\label{sec-bib}
%conceptualization,datacuration,formalanalysis,funding,investigation,methodology,projadm,resources,software,supervision,validation,visualization,writing,review
\begin{contributors}[sec-contributors]
\authorcontribution{Clóvis Henrique Martins Pimentel}[conceptualization,datacuration,formalanalysis,investigation,methodology,projadm,resources,software,validation,visualization,writing,review]
\authorcontribution{Thiago Blanch Pires}[conceptualization,projadm,resources,visualization,writing,review]
\end{contributors}
\end{document}
