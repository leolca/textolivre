\section{Discussion}\label{sec-discussion}

Our findings suggest that audio input does not necessarily fuse with the
subtitles to make for an overall improvement upon language retention as
we had initially hypothesized. It did seem reasonable to have considered
during the hypothesis forming step that the coinciding audio and visual
input of the language (i.e. Spanish audio with Spanish subtitles) would
bolster and reinforce comprehension of the scene. This is because the
audio would ideally complement the reading comprehension. However, the
no audio condition results, with its strikingly dispersed comprehension
scores, revealed that this reasoning is perhaps too optimistic and
elementary. In fact, almost the opposite seemed to happen as the No
Audio parameter concluded with the highest comprehension scores (95.0\%
for Arabic and 84\% for Spanish).

One of the factors that may have subversively impacted the results is
individual stress and anxiety levels during the viewing process.
Participants may have also been slightly discomforted by viewing videos
under conditions in which they are not used to (i.e. No Audio input for
participants who have always watched videos with audio videos). In
addition, the participants may have had an unconscious aversion towards
the voices of the characters and background sounds (music or character
movement and behavior sounds), thus reducing or straining their
attention. A mixed presentation approach using mismatched audio-visual
content goes against the age-old language teaching technique of full
language immersion. Moreover, variation in individual attention spans
could also explain discrepancy in comprehension scores. Cognitive input
levels may have also led to reduced comprehension if participants
experienced cognitive overload due to divided attention efforts. In this
study, cognitive overload is perhaps evidenced from lower fixation
duration as shown in Figure 4. The participants with the audio input
variable did not exceed the fixation duration of 450 ms and when we
conjecture upon this, we can perhaps argue that once the different
ongoing intake information that compete for cognitive attention reaches
an overload point, the participants' gaze then starts to stagger,
falter, or become sporadic. ``Gaze'' data may be misleading as one's
gaze does not always mean that the participant is on task. Some of the
gaze data may cover actual knowledge input time (in our case
``knowledge'' input would be considered actual ``reading time''). Figure
5 shows that reading subtitles only gets you so far since the highest
comprehension score did not take place during the highest reading
duration. Once again, the ``No Audio'' input reigns supreme. The No
Audio (Arabic) variable led to the highest comprehension scores falling
in the 95\% projectile even coupled with a reading time that was below
100,000 ms.

Gaze fatigue may also have had an impact upon the results since
individual gaze stamina will vary amongst participants. According to
\textcite{bafna2021} study, eye gaze fatigue shown in
eye metrics can also be connected to mental fatigue. Mental fatigue
would be a particularly debilitating variable in the comprehension and
retention levels of the participants. Their research highlighted that
indicators related to rapid eye movements, known as saccades, were the
most effective in identifying fatigue \cite{bafna2021}.

Furthermore, the languages themselves may have influenced the results of
the study because each language is characteristically unique, thus the
same parameters with the eye-ear relationship cannot be prescribed
universally. It is suggested that if visual media (movies, shows, etc.)
will be utilized in instructional language training, then discernment
should be applied especially as it relates to the complexity of the
scenes. What is meant by stating ``complexity'' here is the complexity
of the emotional aspect of content as well as the complexity of the
register being used (i.e. formal advanced language vs. colloquial or
basic language). To provide an example, the first video that was shown
had two characters talking about war and arguing over perceived
cowardice. The characters then talked about the intricacies of
fatherhood and affection. For language learners, this type of content
and heavy emotional sentiment that supports the scene may be too
advanced for the language learners to pick up on physical and audio cues
that are universally understood. In addition, language teachers and
scholars need to be cognizant of the differences of individual reader's
physical eye capabilities (mentioned prior in \Cref{sec-literaturereview}) and how readers
read, since not all people are taught the same reading strategies and
approach. Not all languages have the same presentation and pattern of
their parts of speech. A rather notorious difference is the placement of
the verb, which is essentially what informs the reader of the actual
activity being reported in the sentence. Spanish and English consist of
many shared cognates whereas languages that are totally isolated from
each other's language family tree may produce conflicting data, proving
a potential future hypothesis that language pairs also have an impact on
language acquisition through the eye-ear relationship. Students may not
be focusing their eyes on the part of the sentence which would arguably
be the verb. Any supplementary information can be better understood once
the reader knows what the action is.

Another key consideration to be aware of is that the overall ``reading
process could be affected by semantically relevant auditory input in the
context of reading English/L2 subtitles in video'' \cite[p. 260]{liao2022}. The importance of seeing saccade fluctuation because of
purposeful modulation between subtitle and audio input is that
researchers will be able to understand what language learners
essentially ``tune out'' or withdraw their attention from as a response
to the modulations. These results then reinforce assertions relating to
the efficacy of simultaneous audio and subtitle utilization and
hopefully indicate which sort of trend should be adopted in a learning
environment (i.e. language instructors should indeed use L2 subtitles
while L1 audio is present or not). \textcite[p. 260]{liao2022} proclaimed
that their data showed ``that readers adjusted the way they engaged in
the reading of subtitles in response to the varying needs to read the
subtitles in different audio conditions''. The student's own first
language may also affect the reading of the subtitles. Their first
language habits may dictate them to look towards the end of the sentence
or to focus on a particular linguistic unit (i.e. the verb unit is found
to be more prominent in East Asian languages such as Japanese). The
participant's first language was unfortunately not solicited thus we
cannot expand upon this variable. Successful corroboration would appear
inevitable in this experiment's case, however, there is less of an
emphasis on readers' reaction in this study than there is on new
knowledge acquisition for the reader.

There is perhaps a cognitive effort component that can be overlooked if
one is unaware that ``it takes much conscious effort to keep the eyes
fixated while attending to another location'' \cite[p. 85]{schotter2012}. This assertion raises several questions regarding language
acquisition, about the usage of subtitles and other audio-visual aids.
If effort is expended just on fixation during the concurrent saccades of
the eye, then shouldn't the content that is being presented be minimized
to the lowest amount possible and the introduction of new terms or
grammar points also be minimized? Teachers and other language promoters
do not want to cognitively overload or overstimulate the reader (content
receiver); thus, they should be actively engaged in the matter of
subtitles. Educators and even audio-visual translation trainers may want
to consider generating their own subtitles to fit the needs and learning
level of the content receivers (students, trainees, etc.).