\section{Methodology}\label{sec-methodology}

This section presents an overview of the methodology used in the study.
It includes details about the sample and data collection and analysis
processes. The study employs quantitative measures to investigate
subtitle reading and comprehension in novice learners of Spanish and
Arabic. Quantitative data is gathered through eye-tracking measures and
comprehension quizzes to analyze participants\textquotesingle{} reading
patterns and learning outcomes. Our methodological approach differs,
though not to a radical degree, from that of Mart√≠nez and Gomez (2020),
who found that competency in oral understanding of L2 intake increased
through the usage of audio-visual tools and streaming services that use
them. The following subsections of section 3 will provide more detail on
the methodology.

\subsection{Participants}\label{sub-sec-participants}

The study involves 30 participants, comprising 20 native
English-speaking university students as beginner learners of Spanish and
10 beginner learners of Arabic. All participants have studied their
respective language (Spanish or Arabic) for only 1-2 semesters at the
university level. To ensure a broad and unbiased participant pool, the
recruitment phase involves outreach to language instructors.
Participants are aged between 18-40 years to control for age-related
cognitive and oculomotor changes. In accordance with Institutional
Review Board guidelines, informed consent is obtained, along with
assurances of data confidentiality and ethical handling of participant
information. A pilot test with 5 participants is conducted to refine the
study methodology, ensuring clarity in instructions and overall
procedure.

\subsection{Design}\label{sub-sec-design}

The study employs a 2x3 within-subjects design to assess the impact of
subtitle language combined with auditory input. The study manipulated
audio condition (L1 audio, L2 audio, no audio) and subtitle language
(Spanish, Arabic). This design features 2 levels of subtitle language
(Spanish and Arabic) crossed with 3 types of audio exposure (L1 English
Audio, L2 Matched Audio, No Audio). Each native English participant,
either a Spanish learner or an Arabic learner, completes 3 total viewing
conditions specific to their language of study (L2: Spanish/Arabic) (see
\Cref{tab-01}). To ensure the reliability of the results and counter potential
biases, the sequence in which participants encounter these conditions is
randomized, controlling for order effects and fatigue. This strategy is
crucial for reducing learning or adaptation effects across sessions.

Eye-tracking data were collected using the RealEye webcam-based
system, and comprehension was assessed through quizzes. In each
condition, eye-tracking metrics are carefully analyzed to understand
how participants interact with subtitles under varying auditory
conditions. The study focuses on comparing these metrics within each
language group across the audio conditions, aiming to quantify the
linguistic challenges introduced by different audio inputs.

Data analysis involved both global and local analyses of eye
movements. Global analysis examined reading patterns across entire
subtitles, while local analysis focused on specific linguistic units.
Key variables of interest include global subtitle reading strategies
and local lexical processing efficiency, which are evaluated both
within each language group and across different audio conditions.
Statistical analyses, including ANOVAs and t-tests, were conducted to
assess the effects of audio condition and subtitle language on
eye-tracking metrics and comprehension scores.

\begin{table}[!htbp]
    \centering
    \caption{Study Conditions in a 2x3 (Language x Audio Condition)
    Within-Subjects Design}\label{tab-01}
    \begin{threeparttable}
    
    \begin{tabularx}{\textwidth}{cYY}
        \toprule
        Condition & Subtitle Language (L2) & Audio Language \\
        \midrule
        1 & Spanish & Spanish \\
        2 & Spanish & English \\
        3 & Spanish & No Audio \\
        4 & Arabic & Arabic \\
        5 & Arabic & English \\
        6 & Arabic & No Audio \\
        \bottomrule
    \end{tabularx}
	    \begin{tablenotes}
			\item Source: Own elaboration.
	    \end{tablenotes}
    
    \end{threeparttable}
        
\end{table}

\subsection{Material}\label{sub-sec-material}

The study utilizes three one to two-minute video clips from the 2022
animation film \emph{Pinocchio} available on Netflix, with the clips
counterbalanced across audio conditions using a Latin Square design.
Using clips from the same film controls for variability in factors like
emotional engagement or audio-visual (AV) complexity that could
influence processing beyond the linguistic components \cite{winke2013}. Counterbalancing ensures that differences found in eye-tracking
metrics can be attributed to the audio manipulation rather than the
intrinsic features of the videos. The content features simple
conversational dialogues accessible to novice learners, between two main
characters. The simplified narratives target novices while offering
vocabulary learning opportunities. Drawing balanced samples from a
common film maximizes internal validity within the controlled
presentation system tailored to evaluate auditory impacts on
second-language subtitle reading competency development.

The clip\textquotesingle s duration provides an adequate volume of
subtitles for analysis. The subtitles across the Spanish and Arabic
video clips are comparable in length (average of 42.6 characters
including spaces and punctuation), number of lines (one line used for
all subtitles), and readability level based on Coh-Metrix metrics
matching simplified conversational dialogues \cite{graesser2014}.
This duration sustains participant concentration without inducing
fatigue over the experiment. Netflix content ensures consistency in
production quality and vocabulary level using the
platform\textquotesingle s filters. Subtitle parameters of font, size,
and timing are standardized to eliminate visual variability.

\subsection{Apparatus}\label{sub-sec-apparatus}

Participants\textquotesingle{} eye movements are recorded monocularly
using the RealEye webcam-based system. This system, as detailed by
\textcite{lewandowska2019}, leverages advanced AI algorithms for accurate eye
tracking, operating at a sampling rate of about 30 Hz, scalable up to 60
Hz under optimal conditions. The stimuli are displayed on a laptop
screen, effectively simulating real-world online viewing contexts. The
reason why the software was used exclusively on a laptop was because
laptops provide more flexibility for eye-camera balance. Another reason
was that the software itself operated better on a laptop than on a
monitor, in which the eye-tracking software seemed to malfunction more.
The RealEye system\textquotesingle s distinctive feature of functioning
entirely within a web browser, without requiring software downloads,
enhances its accessibility and ease of use in remote settings. Subtitles
are displayed in mono-spaced Courier New font, using 18pt white text
with a 1pt black outline for visibility, ensuring readability across
various viewing conditions. The data filtering process includes only
fixations exceeding a 100ms duration within a 35px dispersion threshold,
which is optimized for online processing and aligns with the RealEye
system\textquotesingle s capabilities \cite{lewandowska2019}.

\subsection{Procedure}\label{sub-sec-procedure}

The study utilizes RealEye\textquotesingle s webcam tracking
functionality for remote eye tracking, streamlining the process for
participants by eliminating the need for software downloads. Instead, a
shareable weblink grants access to the customized portal on personal
laptops and desktops, supporting mainstream Windows and MacOS devices.
The use of personal devices ensures comfort and familiarity for
participants, although mobile devices are not suitable due to their size
limitations. Upon entry into the study portal, participants receive
clear, user-friendly instructions to guide the setup process, including
assistance in positioning both the device and themselves for optimal
capture within the webcam\textquotesingle s view. Visual feedback
confirms suitable face capture, especially the eyes, which may require
minor adjustments in seating or lighting. Once the setup is
satisfactory, participants initiate a calibration procedure, designed to
be brief yet thorough, ensuring accurate tracking.

The procedure involves a randomized sequence presenting 35 distinct
samples, each displaying a tracking dot against a neutral background.
Participants are instructed to maintain focus and precisely click the
target, with the webcam recording gaze spot estimates at each click. A
200-pixel deviation filter is applied between logged coordinates to
ensure data accuracy, discarding any data points indicative of
distraction or improper technique. To further validate the quality of
data and participant engagement, a comprehension quiz is administered.
The quizzes were designed on the target linguistic units and were
conducted with the participants immediately after finishing the
eye-tracking experiment. The utilization of vocabulary tests after the
eye-tracking of subtitles is not novel, as \textcite{bisson2014} also
implemented this method in their study, concluding with interesting
findings that further cement subtitle integration\textquotesingle s
positive efficacy. They found that vocabulary retention was higher with
subtitles than without, and it is anticipated that this
study\textquotesingle s results may corroborate such findings. However,
this study links the comprehension score to the global and local
eye-tracking measures.

\subsection{Analysis}\label{sub-sec-analysis}

The analytical approach encompasses both generalized perspectives
through global subtitle processing examinations and targeted dynamic
evaluations of linguistic unit integration efficiency. Concurrently,
these multi-level analyses aim to explain the complex interplay of
factors influencing subtitle usage and literacy development under varied
cognitive constraints. Specifically, the RealEye software pre-processes
the raw gaze data recorded during full-length viewing trials, leveraging
sophisticated algorithms to accurately classify fixations, saccades, and
blinks. Velocity thresholds combined with spatial dispersion criteria
are optimized for the parameters of the online experiment context.
Subsequently, the resulting scanpath data files are exported to Python
for quantitative aggregation and evaluation using the OpenGazeAnalyzer
toolkit.

For global analyses, areas of interest (AOIs) are designated over the
spatial bounds of subtitle text segments in all video frames. The global
AOIs effectively encompass entire texts of subtitles, excluding non-text
screen regions. Various temporal indicators are then calculated over
these aggregate areas, providing insights into high-level reading
strategies. Metrics such as time to first fixation quantify attentional
prioritization and capture of subtitle content across conditions.
Meanwhile, total fixation duration and counts reflect overall reliance
on subtitles for comprehension, with increased values indicating greater
dependence in the absence of supportive auditory context. Furthermore,
regression rates signify re-reading frequency, with higher rates
implying difficulty in integration necessitating more repetitions. In
essence, these metrics offer macro-level perspectives into attentional
allocation patterns during complex literacy tasks under varied language
proximity and cognitive load constraints.

\begin{small}
\begin{longtable}{p{2cm}p{4cm}p{4cm}l}
\caption{Target Linguistic Units for the three Experiments.}
\label{tab-02}\\
\toprule
Linguistic Unit & English & Spanish & Arabic \\
\midrule
&&&\\
\multicolumn{4}{c}{\textbf{First Experiment ‚Äì L1 English audio and L2 Spanish/Arabic Subtitles}}		\vspace{.2cm}\\
Verb & to fear & Le da miedo & 
\textlang{arabic}{ ŸäÿÆÿßŸÅ } \\
Noun & war & guerra & 
\textlang{arabic}{ ÿßŸÑÿ≠ÿ±ÿ® } \\
Adjective & weak & d√©bil & 
\textlang{arabic}{ ÿ∂ÿπŸäŸÅ } \\
Adverb & sometimes & a veces & 
\textlang{arabic}{ ÿ£ÿ≠ŸäÿßŸÜŸãÿß } \\
Expression & they love you & te quieren mucho & 
\textlang{arabic}{ Ÿäÿ≠ÿ®ŸàŸÜŸÉ } \\
Phrase & every day & todos los d√≠as & 
\textlang{arabic}{ ŸÉŸÑ ŸäŸàŸÖ } \\
Sentence & You will see that I am not a coward. & Le demostrar√© que no soy cobarde. & 
\textlang{arabic}{ ÿ≥ÿ£ÿ´ÿ®ÿ™ ŸÑŸá ÿ£ŸÜŸÜŸä ŸÑÿ≥ÿ™ ÿ¨ÿ®ÿßŸÜŸãÿß. } \\
Question & Are you afraid? & ¬øTienes miedo? & 
\textlang{arabic}{ ŸáŸÑ ÿ£ŸÜÿ™ ÿÆÿßÿ¶ŸÅÿü } \vspace{.2cm}\\

\multicolumn{4}{c}{\textbf{Second Experiment ‚Äì L2 English audio and L2 Spanish/Arabic Subtitles}}\\
&&&\\
Verb & to lose & perdi√≥ & 
\textlang{arabic}{ ŸÅŸÇÿØ } \\
Noun & child & ni√±o & 
\textlang{arabic}{ ÿ∑ŸÅŸÑ } \\
Adjective & painful & doloroso & 
\textlang{arabic}{ ŸÖÿ§ŸÑŸÖ } \\
Adverb & very much & mucho & 
\textlang{arabic}{ ŸÉÿ´Ÿäÿ±Ÿãÿß } \\
Expression & imperfect parents & padres imperfectos & 
\textlang{arabic}{ ÿ¢ÿ®ÿßÿ° ÿ∫Ÿäÿ± ŸÉÿßŸÖŸÑŸäŸÜ } \\
Phrase & for a change & para variar & 
\textlang{arabic}{ ÿπŸÑŸâ ÿ≥ÿ®ŸäŸÑ ÿßŸÑÿ™ÿ∫ŸäŸäÿ± } \\
Sentence & It is something painful that you have to bear & Es algo doloroso que vas arrastrando & 
\textlang{arabic}{ ÿ•ŸÜŸá ÿ¥Ÿäÿ° ŸÖÿ§ŸÑŸÖ Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ÿ™ÿ≠ŸÖŸÑŸá } \\
Question & What is a burden? & ¬øQu√© es una carga? & 
\textlang{arabic}{ ŸÖÿß ŸáŸà ÿßŸÑÿπÿ®ÿ°ÿü } \vspace{.2cm}\\

\multicolumn{4}{c}{\textbf{Third Experiment ‚Äì No audio and L2 Spanish/Arabic Subtitles}}\\
\vspace{.2cm}
Verb & go & Ir√© & 

\textlang{arabic}{ ÿ£ÿ∞Ÿáÿ® } \\

Noun & book & libro & 
\textlang{arabic}{ ŸÉÿ™ÿßÿ® } \\
Adjective & proud & orgulloso & 
\textlang{arabic}{ ŸÅÿÆŸàÿ± } \\
Adverb & sometimes & a veces & 
\textlang{arabic}{ ÿ£ÿ≠ŸäÿßŸÜŸãÿß } \\
Expression & I love it & ¬°Me encanta! & 
\textlang{arabic}{ ÿ£Ÿèÿ≠ÿ®ŸèŸëŸá } \\
Phrase & very good & muy bueno & 
\textlang{arabic}{ ŸÖŸÖŸäÿ≤ ÿ¨ÿØŸãŸëÿß } \\
Sentence & I am going to be like Carlo & Voy a ser igual que Carlo & 
\textlang{arabic}{ ÿ≥ÿ£ŸÉŸàŸÜ ŸÖÿ´ŸÑ ŸÉÿßÿ±ŸÑŸà } \\
Question & Are you ready for school? & ¬øListo para la escuela? & 
\textlang{arabic}{ ŸáŸÑ ÿ£ŸÜÿ™ ŸÖÿ≥ÿ™ÿπÿØ ŸÑŸÑŸÖÿØÿ±ÿ≥ÿ©ÿü } \\
\bottomrule
\source{Own elaboration.}
\end{longtable}
%{\footnotesize{\vspace{-0.4cm}Source: Own elaboration.}}
\end{small}


An important part of the experiment set parameters was to make sure that
the target words that the participants tested on would vary in parts of
speech. This would then increase the difficulty of the task itself so as
not to make stimulation too lulled. There were also phrases included.
\Cref{tab-02} above provides a list of the previously mentioned target words.

Concurrently, local dynamic analyses involve designating precise AOIs
and isolating the set of pre-selected linguistic units embedded within
the subtitles. As enumerated in \Cref{tab-02}, these linguistic units
constitute key lexical items from various grammatical categories that
crucially avoid overlap between the Spanish and Arabic conditions. Over
these local AOIs, precise temporal indicators capture the subtle
dynamics of lexical access and integration efficiency over time.
Measures such as first fixation duration and gaze duration quantify the
initial perceptual fluency of lexical activation when readers first
encounter the critical words. Meanwhile, total reading time incorporates
later processing, indicating the full duration of lexical access
efficiency. Additionally, revisit ratios and regression probability
reflect integration difficulty, with higher values marking regressions
back to words posing encoding challenges. Essentially, these local
metrics reveal the detailed time course over which vocabulary
representations are constructed under the varied constraints imposed by
auditory backing presence and language proximity factors. The local
lexical processing efficiency analyses thus provide targeted dynamic
evaluations, complementing the global generalized perspective.

Finally, supplementary content comprehension questions administered
after each viewing session serve to confirm that attentiveness was
sustained throughout the experiment. Accuracy scores help validate the
eye-tracking data and conclusions drawn while aligning with specific
research questions investigating multimedia learning phenomena. The
combination of global and local eye-tracking analyses, along with
comprehension assessments, provides a comprehensive framework for
understanding the intricate dynamics of subtitle processing and language
acquisition in novice bilingual learners. This multifaceted approach
enables the exploration of the impact of auditory input and language
proximity on attentional allocation, lexical processing efficiency, and
overall comprehension, ultimately contributing to the development of
evidence-based strategies for enhancing language learning through
audiovisual media.







