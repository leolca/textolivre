\section{Metodologia}\label{sec-metodologia}

Para avaliar o desempenho dos modelos no processamento de ambiguidades quanto aos parâmetros da ambiguidade lexical, sintática e semântica, conduzimos tarefas utilizando um conjunto de dados que foi elaborado e analisado pelos autores do presente artigo, um grupo de seis estudantes de Letras e Linguística, cada um com um conhecimento maior em áreas distintas da linguística, como fonética, tradução, sintaxe e análise do discurso. Esses estudantes serão referidos no trabalho como juízes-humanos por se demonstrarem aptos a produzir e julgar de forma adequada os dados, além disso, vale salientar que os dados e resultados das tarefas foram avaliados, posteriormente, por uma coautora que é especialista na área de variação linguística.

Para garantir a consistência e a confiabilidade das frases geradas pelo grupo, foi adotado o procedimento de \textit{inter-annotator agreement}. As sentenças que não receberam consenso absoluto entre os juízes-humanos foram revisadas ou excluídas do corpus. Como resultado, não foi necessário calcular o coeficiente de concordância {\footnote{Ao criar o corpus, as frases foram revisadas por todos os autores do artigo, e apenas aquelas com 100\% de aprovação no procedimento de inter-annotator agreement foram disponibilizadas para os testes, com o objetivo de minimizar a presença de frases problemáticas. As frases geradas pelos modelos também passaram por uma revisão conjunta e foram discutidas entre os autores para garantir consenso absoluto a partir do mesmo procedimento. Por exemplo, na Tabela 4, que corresponde à geração de frases por parte do modelo, a sentença \enquote{O homem viu o acidente com os próprios olhos} contém um componente favorável à criação de ambiguidade sintática, o adjunto \enquote{com os próprios olhos}, no entanto, com base na teoria de Cançado, essa ambiguidade é válida quando acompanhada por uma ambiguidade semântica, que permite o duplo sentido na compreensão pragmática do contexto dentro do enunciado. Já na Tabela 3, a frase \enquote{Pedi o prato principal ao garçom, era filé!} é parte do corpus das sentenças distratoras, pois a estrutura do enunciado fornece informações suficientes para evitar outras interpretações, ou seja, entendemos que \enquote{prato} não se refere ao utensílio doméstico e \enquote{filé} não é um elogio metafórico. Sendo assim, essa frase foi considerada não ambígua pelos seis juízes-humanos e pela especialista em linguística que revisou o trabalho.}}, uma vez que todas as sentenças, tanto as ambíguas quanto as não ambíguas, só foram incluídas após alcançarem 100\% de aprovação. Esse critério rigoroso assegurou que o conjunto de dados utilizado nas tarefas fosse  confiável ao máximo para minimizar divergências.

O experimento foi composto por um grupo de 120 sentenças, distribuídas de forma balanceada entre os três tipos de ambiguidade. Dessas, 60 apresentam algum tipo de ambiguidade, seja semântica, lexical ou sintática. As frases ambíguas foram elaboradas com o objetivo de provocar especificamente um dos três tipos de ambiguidade (semântica, lexical ou sintática). No entanto, é possível que algumas sentenças apresentem mais de um tipo de ambiguidade, que não foi avaliado durante a criação das mesmas. No decorrer de nossa análise, buscamos isolar ao máximo cada frase, de modo que apenas um elemento causador de ambiguidade estivesse presente. Essa abordagem nos permite um controle mais rigoroso das variáveis observadas nos experimentos, uma vez que a ambiguidade é um fenômeno complexo e multifacetado.

É importante ressaltar que, dentro de uma única sentença, podem existir múltiplos fatores que contribuem para a ambiguidade, o que torna a sua identificação e análise ainda mais desafiadoras, então, para um estudo inicial, de caráter pioneiro com LLM's formamos dados linguísticos mais artificiais, mas planejamos trabalhar futuramente com dados provenientes de corpora ou textos reais, incorporando materiais autênticos que nos permitam uma compreensão mais abrangente das ambiguidades linguísticas em contextos variados.

Foram criadas 20 sentenças com ambiguidade lexical — que abrangem casos de homonímia e polissemia, sem distinção de categoria na análise das frases —, 20 sentenças com ambiguidade semântica, nas quais o referente dos pronomes não está claro, e, por fim, 20 sentenças com ambiguidade sintática, envolvendo adjuntos adnominais ou adverbiais ambíguos que provocam duplo sentido devido às diferentes organizações estruturais que a frase pode ter (Tabela \ref{tab:amostras_sentencas}).

As outras 60 sentenças distratoras tiveram sua ambiguidade barrada ao máximo, não sendo verificadas pelos juízes-humanos durante a elaboração dos dados linguísticos. As sentenças distratoras foram criadas com a intenção de evitar qualquer forma de ambiguidade. Da mesma forma que foi feito o julgamento das frases ambíguas, as frases não ambíguas foram avaliadas em um nível de significado mais isolado, sem considerar diversos contextos enunciativos figurativos, na maioria das vezes, nas quais ela poderia fazer sentido, logo assumimos um posicionamento de interpretação mais pragmático linguístico, que corrobora com Cançado. Essas mesmas considerações são válidas para as avaliações feitas em relação às frases que foram geradas pelos modelos de linguagem na Tarefa 4.

Um exemplo para ilustrar essa forma de análise é a frase \enquote{Pedi o prato principal ao garçom, era filé!} (Tabela 3) em que em um contexto muito específico, poderia significar que o cliente pediu o utensílio principal de servir comida e este utensílio era \enquote{filé}, um termo popular para se referir a algo bom, porém, a princípio, a frase foi escrita tendo em vista o significado mais óbvio, que se determinou a partir das pistas interpretativas deixadas dentro da frase na qual um sentido lexical confirmava o outro sem a necessidade de buscar condições exofóricas à sentença que justificassem uma polissemia.





\begin{table}[htpb]
\centering
\begin{threeparttable}
\caption{}
\label{tab:amostras_sentencas}
\begin{tabular}{llp{6cm}}
\toprule
Id & Sentença & Classe \\
\midrule
1 & A rede caiu. & Ambiguidade lexical \\
2 & Ela não gosta da amiga dela. & Ambiguidade semântica \\
3 & O menino viu o incêndio do prédio. & Ambiguidade sintática \\
\bottomrule
\end{tabular}
\source{\url{https:XXXXXXXXX}.}
%\notes{Se necessário, poderá ser adicionada uma nota ao final da tabela.}
\end{threeparttable}
\end{table}

%\source{\url{https://osf.io/u7wre/?view_only=572c74eb4c634d47a02ad25485ea8caa}.}

Para responder as nossas perguntas de pesquisa, foram conduzidas quatro tarefas {\footnote{As respostas dadas pelos modelos de linguagem nas tarefas foram coletadas entre julho de 2023 até janeiro de 2024. Coletas posteriores podem levar a resultados diferentes devido às atualizações dos modelos de linguagem. Para evitar ao máximo o enviesamento dos modelos, as perguntas foram formuladas da forma mais objetiva possível, evitando dar pistas sobre a resposta correta ou sinalizando qual era o resultado esperado. A preocupação em minimizar testes com viés tinha o objetivo de garantir uma avaliação mais precisa do desempenho dos modelos de linguagem, afastar casos de generalização nas respostas, favorecer a imparcialidade dos resultados e alcançar um conjunto de respostas o mais transparente possível.}} distintas com as sentenças criadas. Em todas, foram realizadas coletas duplicadas das interações para cada frase, reiniciando o \textit{console} entre cada coleta para evitar qualquer influência do contexto que pudesse gerar respostas tendenciosas. Essa abordagem permitiu avaliar a consistência dos modelos nas respostas fornecidas.


A tarefa 1 visava identificar se os modelos conseguem detectar a presença de ambiguidade em cada sentença por meio da seguinte instrução: \textbf{A sentença ``[sentença]'' é ambígua? Responda, sim, não ou não sei}. Foram apresentadas individualmente todas as sentenças e registradas as respostas dos modelos, comparando-as com a nossa classificação prévia. As respostas foram cuidadosamente avaliadas quanto à correção e abrangência das explicações fornecidas por seis juízes-humanos que as julgaram independentemente. A partir dos resultados, foi gerada uma matriz de confusão para computar a quantidade de verdadeiros positivos (sentenças que são ambíguas e que os modelos classificaram como ambíguas), falsos positivos (sentenças que não o são e que os modelos classificaram como ambíguas), verdadeiros negativos (sentenças que não são ambíguas e que os modelos assim classificaram como não ambíguas), e falsos negativos (sentenças que são ambíguas e que os modelos classificaram como não ambíguas).


Na tarefa 2, foi realizado um teste para avaliar a capacidade dos modelos em distinguir corretamente entre as três classes de ambiguidade estudadas neste trabalho, formulando a seguinte pergunta para cada modelo: \textbf{``Qual o tipo de ambiguidade?''}. A tarefa consistiu em perguntar qual o tipo de ambiguidade da sentença que foi classificada anteriormente como ambígua ou não ambígua. Na tarefa 3, foi verificada a capacidade dos modelos em desambiguar as sentenças que foram fornecidas a eles. Com esse propósito, foram apresentadas frases que incluem tanto sentenças ambíguas, quanto sentenças não ambíguas, e solicitado aos modelos a seguinte instrução: \textbf{Faça a desambiguação da frase: ``[sentença]''}. A tarefa busca testar a habilidade dos modelos em compreender e interpretar o contexto, escolhendo a interpretação mais apropriada quando a ambiguidade está presente. 

Na tarefa 4, foi avaliada a capacidade dos modelos em gerar frases ambíguas na categoria solicitada. Para isso, pedimos para cada modelo gerar frases da seguinte forma: \textbf{Gere 20 frases com ambiguidade ``[categoria]''}. Em seguida, as respostas obtidas foram avaliadas por juízes-humanos, buscando compreender quão preciso é o ChatGPT o Gemini ao criarem frases que apresentam múltiplas interpretações contextuais.

Para mensurar quantitativamente o desempenho dos modelos, foi utilizada a métrica de acurácia, a qual já é amplamente empregada na área de aprendizado de máquina  \cite{naser2021error, freitag2021funccao}. A acurácia, no contexto da classificação, representa a proporção de frases corretamente classificadas pelos modelos em relação ao total de frases apresentadas na tarefa, como apresentado na equação \ref{eq_1}.

\begin{equation}
    \text{acc} = \frac{\text{Número de previsões corretas}}{\text{Total de previsões}}
    \label{eq_1}
\end{equation}

Todas as sentenças criadas por nós e geradas pelos modelos durante as tarefas estão disponíveis no Apêndice \ref{sec-apendice}. As respostas dos modelos durante as tarefas estão disponíveis para download\footnote{\url{https://docs.google.com/spreadsheets/d/1AOff1GJmh3oWIuKdfBGGHWeox-7HtuHQW0LkaT5yFVI/edit?usp=sharing}} em nosso repositório.


%\footnote{\url{https://osf.io/u7wre/?view_only=572c74eb4c634d47a02ad25485ea8caa}}