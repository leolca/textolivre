\subsection{Quais padrões de ambiguidade os modelos ChatGPT e Bard demonstram conhecer na geração de frases ambíguas?}
\label{resultados-q-5}

A investigação sobre os padrões de ambiguidade usados pelos modelos ChatGPT e Gemini durante a geração de frases ambíguas é abordada por meio da análise das respostas obtidas na tarefa 4. Para conduzir essa análise, seis juízes-humanos qualificados julgaram se as frases geradas pelos modelos continham elementos que induzem ambiguidade perceptível por seres humanos e, em caso afirmativo, se esses elementos se alinhavam corretamente com a categoria de ambiguidade.


\textbf{Ambiguidade Lexical.} Na geração de frases com ambiguidade lexical, tanto o ChatGPT quanto o Gemini revelaram não conhecer os padrões geradores desse tipo de ambiguidade, resultando, em sua maioria, em frases sem qualquer forma de ambiguidade identificada pelos avaliadores humanos. O ChatGPT, em particular, não gerou nenhuma frase com ambiguidade lexical, tendo 18 frases avaliadas sem qualquer notificação de ambiguidade. Por exemplo, foram geradas frases como \enquote{O pássaro voou em direção à árvore mais alta.} e \enquote{Ela encontrou uma bela maçã na floresta.}, em que para um observador humano não há ambiguidade, revelando clareza na expressão. Em apenas duas das frases geradas, foram identificados padrões de adjunto ambíguo, referindo-se a ambiguidade estrutural ou sintática, não se enquadrando corretamente na categoria de ambiguidade lexical.

O Gemini também enfrentou dificuldades ao gerar frases com ambiguidade lexical. Das 20 frases, em 14 delas nenhum avaliador humano conseguiu identificar qualquer tipo de ambiguidade, resultando em uma taxa de sucesso de apenas 20\% para o Gemini e de 0\% para o ChatGPT. Um exemplo é a frase \enquote{O advogado defendeu o criminoso.} que não apresenta ambiguidade perceptível por um humano, pois com o contexto da frase é facilmente inferido que o advogado defendeu legalmente o criminoso. Foram observados apenas dois casos em que a ambiguidade residia na homonímia e polissemia, como na frase \enquote{O professor ensinou a classe.}. Além disso, o Gemini, em alguns casos, utilizou padrões que se alinham com a ambiguidade situacional, que é considerada parte da ambiguidade lexical por Cançado, por exemplo ao sugerir a frase \enquote{A casa está vazia} acompanhada da explicação que a casa pode estar vazia de pessoas ou de móveis. O desempenho nesta tarefa reforça o resultado de os modelos já possuem uma certa compreensão de ambiguidade, mas que ambos os modelos ainda enfrentam desafios na geração de frases em que o elemento gerador de ambiguidade é um item lexical.


\textbf{Ambiguidade Sintática.}
Na análise da geração de frases com ambiguidade sintática, o ChatGPT apresentou um desempenho relativamente superior, com uma taxa de sucesso de 65\%. Ou seja, das 20 frases geradas, 13 efetivamente incorporaram ambiguidade sintática por meio de complementos sintáticos, adjuntos adverbiais ou adnominais ambíguos, demonstrando que o modelo conseguiu aprender tais padrões de forma mais satisfatória. Por exemplo, na frase \enquote{Ela viu o homem com o telescópio,} o ChatGPT explorou a ambiguidade gerada pelo complemento sintático \enquote{com o telescópio,} permitindo interpretações tanto de ela ter utilizado um telescópio para ver o homem quanto de o homem estar com um telescópio quando foi visto. O modelo também utilizou ambiguidade oriunda de adjuntos adverbiais e adnominais, como em \enquote{A mãe deu um presente para a filha com uma fita bonita,} na qual o adjunto \enquote{com uma fita bonita} possibilita interpretações sobre o presente ter uma fita bonita ou a filha estar com uma fita bonita. Estes são os casos clássicos de ambiguidade sintática, amplamente descritos na literatura e explicados a partir dos princípios da aposição mínima e da aposição local \cite{maiadimensoes}, com descrições no português brasileiro \cite{maia2003processamento, maia2004compreensao, brito2013processamento}.

Apesar dos resultados satisfatórios obtidos na geração das frases, foram encontradas inconsistências na interpretação do elemento gerador da ambiguidade em 7 das 20 frases, mesmo quando a ambiguidade sintática estava de fato presente. Além disso, o modelo gerou 6 frases sem identificação de qualquer tipo de ambiguidade pelos juízes-humanos. Em 20\% das frases, o ChatGPT personificou elementos não-humanos, prejudicando a interpretação correta. Por exemplo, na frase \enquote{O jogador marcou um gol com a camisa amarela.}, o ChatGPT colocou o adjunto, gerando uma sentença com potencial de ser sintaticamente ambígua, no entanto, a compreensão de senso comum barra a possibilidade da frase ter ambiguidade para humanos, pois a interpretação mais intuitiva linguisticamente será que o jogador estava vestindo uma camisa amarela e descartará o sentido de que o gol vestia uma camisa amarela.

Embora a nossa comunicação seja potencialmente ambígua, a ambiguidade não parece ser um problema, pois quando há interferência, a ambiguidade é resolvida com o esclarecimento, reparo ou correção. A IA atua como o analista, como explicam \cite{freitag2021gramatica}, que dispõem de um grande conjunto de dados desprovidos de contexto, e que portanto são propensos a gerar ambiguidade na compreensão, mas que só existem do ponto de vista da IA.

O Gemini demonstrou um conhecimento ainda mais limitado dos padrões geradores de ambiguidade sintática, gerando apenas 4 das 20 frases com adjuntos adnominais e adverbiais como fontes de ambiguidade, resultando em uma taxa de acertos de apenas 20\%. Das 16 frases sem ambiguidade sintática, 11 não apresentam nenhum tipo de ambiguidade, como em \enquote{O homem comprou o livro que estava na prateleira,} em que erroneamente ela atribuiu ambiguidade à expressão \enquote{na prateleira.}, algo que um ser humano não faria, embora seja possível por conta dos princípios da aposição mínima e da aposição local \cite{brito2013processamento, maiadimensoes}. Além disso, em quatro sentenças, o Gemini confundiu o uso de palavras polissêmicas e homônimas, classificando erroneamente a ambiguidade lexical como sintática. Também é notável que, ao contrário do ChatGPT, o Gemini não incorporou a personificação de elementos inanimados em suas gerações.

Em 80\% das frases geradas, as explicações fornecidas pelo Gemini foram incoerentes demonstrando que o modelo ainda não consegue explicar corretamente a causa da ambiguidade, como na frase \enquote{O homem viu a mulher na janela,} onde atribuiu a ambiguidade ao verbo \enquote{ver,} ignorando que a verdadeira fonte era o adjunto \enquote{na janela,} permitindo que o homem ou a mulher estivessem na janela. Outro problema observado foi a dificuldade dos modelos em lidar com ambiguidades sintáticas envolvendo a concordância de gênero, como no caso de adjuntos que podem concordar tanto com sujeito como o objeto. Na frase \enquote{O rapaz encontrou a carteira perdida no chão do parque}, o adjetivo \enquote{perdida} só pode ocupar a função de adjunto adnominal por se referir à carteira, objeto da sentença, devido à flexão de gênero. O termo não pode ser adjunto adverbial do verbo \enquote{encontrar}, porque não concorda com o sujeito do verbo, que está no masculino. Essa ocorrência dificilmente ocorreria em inglês, por ser uma língua sem uso generalizado de gênero, logo essa ambiguidade seria mantida com o uso do termo \enquote{lost}. Isso mostra que os resultados poderiam ter sido melhores se as tarefas fossem realizadas em inglês, já que o modelo é majoritariamente treinado nessa língua e evidencia a necessidade de mais estudos e investimento sobre os dados linguísticos dos modelos de linguagem na língua portuguesa.


\textbf{Ambiguidade Semântica.}
É importante ressaltar que a ambiguidade semântica, em determinados referenciais teóricos, pode se assemelhar à ambiguidade lexical ou, em alguns casos, não é reconhecida como uma categoria distinta, resultando em uma linha tênue de separação entre ela e outros tipos de ambiguidade \cite{zavaglia2003ambiguidade}. Contudo, os resultados gerados pelo ChatGPT e pelo Gemini não se manifestam apenas na mistura de diferentes padrões de ambiguidade. Em vez disso, destaca-se na geração de frases que carecem de qualquer ambiguidade, as quais, foram consideradas como possuidoras de ambiguidade semântica. O ChatGPT, por exemplo, produziu 7 frases que não apresentaram ambiguidade para os juízes-humanos, enquanto o Gemini gerou 15 frases nessas condições. As demais sentenças, em sua maioria, foram geradas com base em padrões de adjunto ambíguo ou, residindo apenas em elementos lexicais.

Ambos os modelos geraram frases semelhantes ou idênticas às produzidas para ambiguidade sintática. O ChatGPT gerou a sentença \enquote{Ela viu o homem com o telescópio.}, e o Gemini, \enquote{A menina viu o homem com o binóculo}. Ambas continham adjunto ambíguo, e padrões similares foram reproduzidos quando solicitadas frases com ambiguidade sintática, evidenciando a falta de distinção clara entre os dois tipos de ambiguidade por parte dessas versões dos modelos.

\textbf{Caso particular.} Apesar do Manual de Semântica \cite{canccado2005manual} não tratar diretamente de substantivos compostos cristalizados pelo uso social e do nosso cotidiano, vale destacar duas sentenças geradas por ambos os modelos na Tarefa 4. O ChatGPT gerou a frase \enquote{Ele pegou a maçã verde do chão} e o Gemini produziu \enquote{O carro está parado no sinal vermelho.} Ambas as frases podem ser consideradas ambíguas devido aos termos compostos \enquote{maçã verde} e \enquote{sinal vermelho}. No primeiro caso, \enquote{maçã verde} pode se referir tanto à cor da fruta quanto a uma variedade específica de maçã, criando uma ambiguidade lexical. Já no segundo exemplo, \enquote{sinal vermelho} pode ser interpretado como a luz vermelha do semáforo, um alerta de perigo ou o poste do semáforo pintado de vermelho, resultando em diferentes interpretações.  

Esses dois exemplos evidenciam um padrão reconhecido pelos modelos na geração de sentenças classificadas como ambiguidade semântica, que classificamos como lexical no nosso julgamento, por estar em conformidade com o que a autora propõe em sua taxonomia em relação aos itens com múltiplas inferências de significado. Os componentes lexicais \enquote{maçã verde} e \enquote{sinal vermelho} provocam ambiguidade ao apresentarem mais de um sentido possível para o termo, mas essa distinção só é válida para fins de sistematização da análise dos dados. Reforçamos que não presumíamos que os modelos deveriam seguir a classificação adotada no estudos. Por conseguinte, não julgamos a nomenclatura como correta ou errada, mas somente a explicação que acompanhava as frases que foram geradas. Em vista dessas considerações, tal evidência pode servir como uma provocação para futuras pesquisas sobre a relação entre ambiguidade e modelos de linguagem, especialmente em como esses modelos lidam com a interpretação de termos cristalizados que carregam múltiplos sentidos lexicais.